# Port 8000 - Production Monitoring Setup

**Version:** 1.0
**Status:** Ready for Production Deployment
**Date:** 2025-10-25

---

## Overview

Complete monitoring and alerting configuration for Port 8000 in production.

Includes:
- âœ… Error rate alerting (>1% triggers warning)
- âœ… Response time alerting (>1s P95 triggers warning)
- âœ… Database alerting (connection pool, slow queries)
- âœ… Memory alerting (>80% triggers warning)
- âœ… Uptime monitoring (health checks every 60 seconds)
- âœ… Alert escalation procedures
- âœ… 4 operational dashboards
- âœ… 10 alert rules
- âœ… Integration with Slack & PagerDuty

---

## Alerting Thresholds

### Error Rate Alerts

**CRITICAL: Error rate >5%**
- Duration: 1 minute
- Action: Page on-call engineer immediately
- Response: <5 minutes
- Escalation: Manager if no response in 15 minutes

**Example trigger:**
```
rate(http_requests_total{status=~"5.."}[5m]) > 0.05
```

**WARNING: Error rate >1%**
- Duration: 5 minutes
- Action: Slack notification to #deia-warnings
- Response: <15 minutes
- Investigation: Check logs, identify cause

**Example trigger:**
```
rate(http_requests_total{status=~"5.."}[5m]) > 0.01
```

---

### Response Time Alerts

**CRITICAL: P95 response time >4 seconds**
- Duration: 5 minutes
- Action: Page on-call engineer
- Response: <5 minutes
- Investigation: Database slow queries? Resource exhaustion?

**Example trigger:**
```
histogram_quantile(0.95, http_request_duration_seconds) > 4
```

**WARNING: P95 response time >1 second**
- Duration: 10 minutes
- Action: Slack notification
- Response: <30 minutes
- Investigation: Monitor trends, capacity planning

**Example trigger:**
```
histogram_quantile(0.95, http_request_duration_seconds) > 1
```

---

### Database Alerts

**CRITICAL: Connection pool >80% utilization**
- Duration: 2 minutes
- Current threshold: 16/20 connections (tested: breaks at 19/20)
- Action: Page on-call engineer
- Response: <5 minutes
- Solution: Increase pool to 40, restart app

**Example trigger:**
```
db_connection_pool_active > 16
```

**WARNING: Slow query detected (>1 second)**
- Duration: 1 minute (single query)
- Action: Log alert, monitor
- Response: <15 minutes
- Investigation: Check query plan, add indexes

**Example trigger:**
```
pg_stat_statements_mean_time > 1000
```

---

### Memory Alerts

**CRITICAL: Memory >95% of available**
- Duration: 1 minute
- Current system: >1.9GB on 2GB system
- Action: Page on-call engineer
- Response: <5 minutes
- Solution: Restart service, investigate leak

**Example trigger:**
```
process_resident_memory_bytes > 1900000000
```

**WARNING: Memory >80%**
- Duration: 5 minutes
- Current system: >1.6GB on 2GB
- Action: Slack notification
- Response: <30 minutes
- Investigation: Memory leak? Cache growing?

**Example trigger:**
```
process_resident_memory_bytes > 1600000000
```

---

### Uptime Monitoring

**Health Check Every 60 Seconds:**
```
curl -f http://localhost:8000/health || exit 1
```

**Expected Response:**
```json
{
  "status": "healthy",
  "timestamp": "2025-10-25T20:00:00Z",
  "version": "1.0.0",
  "uptime": "12:34:56"
}
```

**Alert: Service Down**
- Response code: Not 200
- Duration: 1 minute
- Action: Page on-call engineer immediately
- Response: <2 minutes

**Alert: Database Unreachable**
- Database health endpoint returns non-200
- Duration: 30 seconds
- Action: Page on-call engineer
- Response: <5 minutes

---

## Dashboard Configuration

### Dashboard 1: System Overview

**Purpose:** At-a-glance system health

**Panels:**
1. Uptime (green/red indicator)
   - Shows if service is running
   - Updates every 30 seconds

2. Response Time P95 (gauge)
   - Safe: <500ms (green)
   - Warning: 500ms-2s (yellow)
   - Critical: >2s (red)

3. Error Rate (graph)
   - Shows errors per minute
   - Threshold lines at 1% and 5%

4. Active Requests (gauge)
   - Number of concurrent requests
   - Helps identify traffic patterns

5. Database Connections (gauge)
   - Shows pool utilization
   - Safe: <10/20 (green)
   - Warning: 10-16/20 (yellow)
   - Critical: >16/20 (red)

6. Memory Usage (gauge)
   - Show RAM utilization
   - Safe: <1.2GB (green)
   - Warning: 1.2-1.6GB (yellow)
   - Critical: >1.6GB (red)

---

### Dashboard 2: Request Metrics

**Purpose:** Detailed API performance analysis

**Panels:**
1. Request Rate (time series)
   - Requests per second over time
   - Identify traffic spikes

2. Response Time Distribution (percentiles)
   - P50, P95, P99 over time
   - Track performance trends

3. Request Size (gauge)
   - Average request size
   - Monitor for bloat

4. Response Size (gauge)
   - Average response size
   - Monitor for bloat

5. Top Slow Endpoints (bar chart)
   - Which endpoints are slowest
   - Target for optimization

6. Error Rate by Endpoint (bar chart)
   - Which endpoints error most
   - Identify problem areas

---

### Dashboard 3: Database Performance

**Purpose:** Database health and performance

**Panels:**
1. Query Execution Time (percentiles)
   - P50, P95, P99 query times
   - Identify slow queries

2. Connection Pool Status (gauge)
   - Active connections: 3/20
   - Healthy vs warning vs critical

3. Slow Queries (table)
   - List of queries taking >1 second
   - Show frequency and avg time

4. Database Size (gauge)
   - Total database size
   - Track growth over time

5. Cache Hit Ratio (gauge)
   - % of queries hitting cache
   - Higher is better

6. Transaction Rate (time series)
   - Transactions per second
   - Monitor for bottlenecks

---

### Dashboard 4: Operational Metrics

**Purpose:** Infrastructure and operational health

**Panels:**
1. CPU Usage (gauge)
   - Current CPU utilization
   - Target: <70% sustained

2. Memory Usage (gauge)
   - Current RAM utilization
   - Target: <1.5GB

3. Disk Space (gauge)
   - Free disk space
   - Alert if <20GB free

4. Network I/O (time series)
   - Bytes in/out per second
   - Monitor for saturation

5. Active Bot Instances (gauge)
   - Number of bots running
   - Shows scaling activity

6. Uptime (counter)
   - Days since last restart
   - Track stability

---

## Alert Notification Configuration

### Slack Integration

**Channel:** #deia-alerts

**Alert Severity Mapping:**
- CRITICAL â†’ ðŸ”´ Red, mentions @deia-oncall
- WARNING â†’ ðŸŸ¡ Yellow, @deia-engineering
- INFO â†’ ðŸŸ¢ Green, posted without mention

**Example Alert Message:**
```
ðŸ”´ CRITICAL: Port 8000 Response Time High
Severity: CRITICAL
Alert: HighResponseTime
Value: P95 = 3.8 seconds (threshold: 1s)
Duration: 5 minutes
Service: http://localhost:8000/health
Runbook: wiki.internal/runbook/slow-response

Action Required: <5 minutes
Recommended: Check database, restart if necessary
```

---

### PagerDuty Integration

**CRITICAL alerts trigger PagerDuty incidents**

**Escalation Policy:**
1. Primary on-call engineer (5 min)
2. Secondary on-call engineer (10 min)
3. DevOps lead (15 min)
4. Engineering manager (20 min)

**Auto-resolve:** When alert condition clears for 5 minutes

---

## Health Check Endpoints

### Endpoint 1: `/health`

**Response:**
```json
{
  "status": "healthy",
  "timestamp": "2025-10-25T20:00:00Z",
  "version": "1.0.0",
  "uptime": "12:34:56"
}
```

**Interval:** Every 60 seconds
**Timeout:** 5 seconds
**Failure threshold:** 2 consecutive failures

---

### Endpoint 2: `/health/db`

**Response:**
```json
{
  "status": "connected",
  "database": "deia_prod",
  "latency_ms": 8,
  "tables_accessible": 12
}
```

**Interval:** Every 60 seconds
**Timeout:** 5 seconds
**Alert if:** Fails for >1 minute

---

### Endpoint 3: `/health/cache`

**Response:**
```json
{
  "status": "healthy",
  "provider": "redis",
  "latency_ms": 2,
  "memory_used_mb": 45
}
```

**Interval:** Every 60 seconds
**Timeout:** 3 seconds
**Alert if:** Fails (but non-critical)

---

## Monitoring Best Practices

### Daily Checks (5 minutes)
1. Review uptime: Should be 100% (or near)
2. Check error rate: Should be <0.5%
3. Verify backups: Completed within last 24h

### Weekly Checks (15 minutes)
1. Review performance trends
2. Check capacity utilization
3. Identify slow endpoints
4. Plan optimizations

### Monthly Checks (1 hour)
1. Full health audit
2. Review all metrics
3. Capacity planning
4. Update runbooks if needed

---

## Common Issues & Monitoring

### Issue 1: High Error Rate

**Alert:** WARNING at 1%, CRITICAL at 5%

**Investigation Steps:**
1. Check `/metrics` endpoint for error patterns
2. Review application logs for errors
3. Check database connectivity
4. Verify Ollama is running

**Common Causes:**
- Database connection issues
- Ollama service down
- Invalid user input
- Rate limiting triggered

**Resolution:**
- Restart database: `systemctl restart postgresql`
- Restart app: `systemctl restart deia-bot-controller`
- Check logs: `journalctl -u deia-bot-controller -f`

---

### Issue 2: High Response Time

**Alert:** WARNING at 1s, CRITICAL at 4s (P95)

**Investigation Steps:**
1. Check database slow query log
2. Monitor CPU and memory usage
3. Check if bot instances overloaded
4. Verify network latency

**Common Causes:**
- Slow database queries
- Resource exhaustion (CPU/memory)
- Too many concurrent requests
- Ollama inference taking long

**Resolution:**
- Add database indexes
- Increase server resources
- Restart service
- Optimize bot config

---

### Issue 3: Database Connection Pool Exhaustion

**Alert:** WARNING at >10 connections (80%), CRITICAL at >16 (95%)

**Investigation Steps:**
1. List open connections: `psql -c "SELECT count(*) FROM pg_stat_activity;"`
2. Identify slow queries
3. Check for connection leaks

**Common Causes:**
- Slow queries holding connections
- Connection leak in application
- Too many concurrent requests
- Database settings too conservative

**Resolution:**
1. Increase pool size (config change)
2. Kill slow queries if necessary
3. Restart application
4. Monitor closely for 1 week

---

### Issue 4: Memory Leak

**Alert:** WARNING at >1.6GB, CRITICAL at >1.9GB

**Investigation Steps:**
1. Monitor memory trend over time
2. Check for large objects in cache
3. Look for unbounded growth

**Common Causes:**
- Unbounded cache growth
- Message queue not draining
- Connection leaks

**Resolution:**
1. Restart service
2. Clear cache: `redis-cli FLUSHDB`
3. Investigate root cause
4. Monitor in following days

---

## Alert Tuning (First Week)

During first week of production, monitor alerts:

**May need tuning:**
- Response time threshold (currently 1s P95)
- Error rate threshold (currently 1%)
- Database connection threshold (currently 16/20)
- Memory threshold (currently 1.6GB)

**Adjust based on:**
- Actual traffic patterns
- Observed false positive rate
- Business requirements
- Resource availability

---

## Monitoring Checklist

- [x] Prometheus configured with 6 scrape jobs
- [x] Grafana dashboards created (4 dashboards)
- [x] Alert rules configured (10 rules)
- [x] Alertmanager configured
- [x] Slack integration configured
- [x] PagerDuty integration configured
- [x] Health check endpoints working
- [x] Metrics collection verified
- [x] Log aggregation configured
- [x] On-call procedures documented

**Total: 10/10 items configured** âœ…

---

## What to Monitor During Deployment

**First Hour:**
- Uptime: Should be 100%
- Error rate: Should be <0.5%
- Response time: Should be <200ms
- Database connections: Should be <5/20

**First Day:**
- Memory usage: Should stabilize
- Error patterns: Should be consistent
- Traffic patterns: Watch for spikes
- Backup completion: Should succeed

**First Week:**
- Average response time
- Error rate trends
- Database pool usage
- Resource utilization
- User complaints

---

## Success Criteria

âœ… All monitoring configured and tested
âœ… All alerts firing correctly
âœ… All dashboards displaying data
âœ… Escalation procedures in place
âœ… On-call team ready
âœ… Runbooks prepared

**Status: PRODUCTION MONITORING READY** âœ…

---

**Production Monitoring Setup Complete**

Port 8000 is fully monitored and ready for production deployment.

**Next Step:** Execute deployment following DEPLOYMENT-READINESS-GUIDE.md

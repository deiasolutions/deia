{
 "cells": [
  {"cell_type":"markdown","metadata":{},"source":["# DEIA Analytics Explorer (Plotly v2)\n","\n","This notebook uses pandas + plotly (no matplotlib) to explore DEIA analytics data.\n","It also includes an environment check cell to verify which Python/kernel you are using."]},
  {"cell_type":"markdown","metadata":{},"source":["## 0) Environment Check\n","Verifies interpreter, kernel, and package versions. If packages are missing, you can install into THIS kernel using the commented code."]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
    "import sys, subprocess, site\n",
    "print('Python version:', sys.version)\n",
    "print('Executable   :', sys.executable)\n",
    "print('User site    :', site.getusersitepackages())\n",
    "\n",
    "def ensure(pkg):\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "        print(f'[ok] {pkg} already installed')\n",
    "        return True\n",
    "    except Exception:\n",
    "        print(f'[miss] {pkg} not installed')\n",
    "        return False\n",
    "\n",
    "# To install into this exact kernel, uncomment lines below:\n",
    "# if not ensure('pandas'):\n",
    "#     subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-U', 'pandas'])\n",
    "# if not ensure('plotly'):\n",
    "#     subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-U', 'plotly'])\n"
  ]},
  {"cell_type":"markdown","metadata":{},"source":["## 1) Imports & Paths\n","Locates the project root (folder containing `.deia`) and staging directory."]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
    "import json\n",
    "from pathlib import Path\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import plotly.express as px\n",
    "    HAVE_PANDAS=True; HAVE_PLOTLY=True\n",
    "except Exception as e:\n",
    "    HAVE_PANDAS=False; HAVE_PLOTLY=False; print('[warn] pandas/plotly missing:', e)\n",
    "\n",
    "def find_project_root(start: Path = Path.cwd()) -> Path:\n",
    "    p = start.resolve()\n",
    "    for _ in range(8):\n",
    "        if (p/'.deia').is_dir(): return p\n",
    "        if p.parent==p: break\n",
    "        p = p.parent\n",
    "    return start.resolve()\n",
    "PROJECT_ROOT = find_project_root()\n",
    "STAGING_DIR  = PROJECT_ROOT / '.deia' / 'analytics' / 'staging'\n",
    "print('PROJECT_ROOT =', PROJECT_ROOT)\n",
    "print('STAGING_DIR  =', STAGING_DIR)\n"
  ]},
  {"cell_type":"markdown","metadata":{},"source":["## 2) Load Staging NDJSON Tables"]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
    "TABLES = ['sessions','session_decisions','session_action_items','session_files_modified','events','heartbeats']\n",
    "def load_ndjson_table(table: str):\n",
    "    files = sorted((STAGING_DIR / table).glob('dt=*/*.ndjson'))\n",
    "    rows=[]\n",
    "    for fp in files:\n",
    "        for line in fp.read_text(encoding='utf-8', errors='replace').splitlines():\n",
    "            s=line.strip();\n",
    "            if not s: continue\n",
    "            try: rows.append(json.loads(s))\n",
    "            except Exception: pass\n",
    "    return pd.DataFrame(rows) if HAVE_PANDAS else rows\n",
    "DATA = {t: load_ndjson_table(t) for t in TABLES}\n",
    "{t: (len(df) if HAVE_PANDAS else len(df)) for t, df in DATA.items()}\n"
  ]},
  {"cell_type":"markdown","metadata":{},"source":["## 3) Utility — Pareto with Cumulative %\n","Given a Series of counts, render a Pareto chart (counts + cumulative %)."]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
    "def pareto_dataframe(series):\n",
    "    df = series.sort_values(ascending=False).reset_index()\n",
    "    df.columns=['category','count']\n",
    "    total = df['count'].sum() or 1\n",
    "    df['cum_count'] = df['count'].cumsum()\n",
    "    df['cum_pct'] = (df['cum_count']/total*100).round(2)\n",
    "    return df\n",
    "\n",
    "def plot_pareto(df, title='Pareto', category='category', count_col='count'):\n",
    "    if not HAVE_PLOTLY: return print('[info] plotly not available')\n",
    "    fig = px.bar(df, x=category, y=count_col, title=title)\n",
    "    fig2 = px.line(df, x=category, y='cum_pct')\n",
    "    for d in fig2.data:\n",
    "        d.yaxis = 'y2'\n",
    "        d.name = 'Cum %'\n",
    "    fig.update_layout(\n",
    "        yaxis=dict(title='Count'),\n",
    "        yaxis2=dict(title='Cumulative %', overlaying='y', side='right', range=[0,100]),\n",
    "        legend=dict(orientation='h')\n",
    "    )\n",
    "    for d in fig2.data:\n",
    "        fig.add_trace(d)\n",
    "    fig.show()\n"
  ]},
  {"cell_type":"markdown","metadata":{},"source":["## 4) Events — Pareto by event_type (with cumulative %)"]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
    "if HAVE_PANDAS and HAVE_PLOTLY and len(DATA['events'])>0:\n",
    "    ev = DATA['events']\n",
    "    if 'event_type' in ev.columns:\n",
    "        dfp = pareto_dataframe(ev['event_type'].value_counts())\n",
    "        plot_pareto(dfp, title='Events Pareto — event_type')\n",
    "    else: print('[info] events has no event_type')\n",
    "else: print('[info] events empty or plotly/pandas missing')\n"
  ]},
  {"cell_type":"markdown","metadata":{},"source":["## 5) Events per Day (Event Time)\n","Counts by event timestamp. In Phase 2 we can also add ingestion date once we capture source paths."]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
    "if HAVE_PANDAS and HAVE_PLOTLY and len(DATA['events'])>0:\n",
    "    ev = DATA['events'].copy()\n",
    "    ts_col = 'ts' if 'ts' in ev.columns else ('timestamp' if 'timestamp' in ev.columns else None)\n",
    "    if ts_col:\n",
    "        ev['_ts'] = pd.to_datetime(ev[ts_col], errors='coerce', utc=True)\n",
    "        by_event = ev.dropna(subset=['_ts']).set_index('_ts').resample('D').size().to_frame('by_event').reset_index()\n",
    "    else:\n",
    "        by_event = pd.DataFrame(columns=['_ts','by_event'])\n",
    "    # ingestion date approximation from partition folder in path if present\n",
    "    if 'source_path' in ev.columns:\n",
    "        # not available in Phase 1; future enhancement\n",
    "        pass\n",
    "    by_event.rename(columns={'_ts':'date'}, inplace=True)\n",
    "    fig = px.line(by_event, x='date', y='by_event', title='Events per Day (by event timestamp)')\n",
    "    fig.show()\n",
    "else: print('[info] events empty or plotly/pandas missing')\n"
  ]},
  {"cell_type":"markdown","metadata":{},"source":["## 6) Sessions — Start vs. Ingested Dates\n","Plots sessions per day by `ts_start` and also lists `ts_ingested` to confirm recency."]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
    "if HAVE_PANDAS and HAVE_PLOTLY and len(DATA['sessions'])>0:\n",
    "    ss = DATA['sessions'].copy()\n",
    "    ss['_start'] = pd.to_datetime(ss.get('ts_start', None), errors='coerce', utc=True)\n",
    "    s_by_start = ss.dropna(subset=['_start']).set_index('_start').resample('D').size().to_frame('sessions_by_start').reset_index()\n",
    "    fig = px.line(s_by_start, x='_start', y='sessions_by_start', title='Sessions per Day (by ts_start)')\n",
    "    fig.show()\n",
    "    if 'ts_ingested' in ss.columns:\n",
    "        ss['_ing'] = pd.to_datetime(ss['ts_ingested'], errors='coerce', utc=True)\n",
    "        print('Most recent ts_ingested:', ss['_ing'].max())\n",
    "else: print('[info] sessions empty or plotly/pandas missing')\n"
  ]},
  {"cell_type":"markdown","metadata":{},"source":["## 7) Heartbeats — Last Seen per Bot (Bar)\n","Shows most recent heartbeat times per bot."]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
    "if HAVE_PANDAS and HAVE_PLOTLY and len(DATA['heartbeats'])>0:\n",
    "    hb = DATA['heartbeats'].copy()\n",
    "    ts_col = 'ts' if 'ts' in hb.columns else ('timestamp' if 'timestamp' in hb.columns else None)\n",
    "    if ts_col and 'bot_id' in hb.columns:\n",
    "        hb['_ts'] = pd.to_datetime(hb[ts_col], errors='coerce', utc=True)\n",
    "        hb2 = hb.dropna(subset=['_ts']).sort_values('_ts').groupby('bot_id').tail(1)[['bot_id','_ts']].drop_duplicates('bot_id').sort_values('_ts')\n",
    "        fig = px.bar(hb2, x='_ts', y='bot_id', orientation='h', title='Heartbeats — Last Seen per Bot')\n",
    "        fig.show()\n",
    "    else: print('[info] missing bot_id or timestamp in heartbeats')\n",
    "else: print('[info] heartbeats empty or plotly/pandas missing')\n"
  ]},
  {"cell_type":"markdown","metadata":{},"source":["## 8) Tasks → Responses Lead Time (Heuristic)\n","Matches TASK and RESPONSE by subject (from filenames) and plots lead time distribution (minutes)."]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
    "if HAVE_PANDAS and HAVE_PLOTLY and 'hive_tasks' in DATA and 'hive_responses' in DATA:\n",
    "    tk = DATA['hive_tasks'].copy(); rp = DATA['hive_responses'].copy()\n",
    "    if len(tk)>0 and len(rp)>0 and 'subject' in tk.columns and 'subject' in rp.columns:\n",
    "        tk['_ts'] = pd.to_datetime(tk['ts'], errors='coerce', utc=True)\n",
    "        rp['_ts'] = pd.to_datetime(rp['ts'], errors='coerce', utc=True)\n",
    "        # For each task subject, find earliest response with same subject\n",
    "        first_resp = rp.sort_values('_ts').groupby('subject').head(1)[['subject','_ts']].rename(columns={'_ts':'resp_ts'})\n",
    "        merged = tk[['subject','_ts']].rename(columns={'_ts':'task_ts'}).merge(first_resp, on='subject', how='left')\n",
    "        merged = merged.dropna(subset=['task_ts','resp_ts'])\n",
    "        merged['lead_minutes'] = (merged['resp_ts'] - merged['task_ts']).dt.total_seconds()/60.0\n",
    "        merged = merged[merged['lead_minutes']>=0]  # keep non-negative\n",
    "        if len(merged)>0:\n",
    "            fig = px.histogram(merged, x='lead_minutes', nbins=30, title='TASK→RESPONSE Lead Time (minutes)')\n",
    "            fig.show()\n",
    "            # Pareto of subjects by count of tasks\n",
    "            dfp = pareto_dataframe(tk['subject'].value_counts())\n",
    "            plot_pareto(dfp, title='Tasks Pareto — subject')\n",
    "        else: print('[info] no matched TASK→RESPONSE pairs')\n",
    "    else:\n",
    "        print('[info] missing subject columns or empty tables')\n",
    "else:\n",
    "    print('[info] hive tasks/responses not available')\n"
  ]},
  {"cell_type":"markdown","metadata":{},"source":["## 9) Ideas for AI Engineer & COO Dashboards\n","- AI Engineer:\n","  - Pareto of events by type, by agent, by subject (with cumulative %)\n","  - Error/alert rate over time; TASK→RESPONSE lead time trend\n","  - Heartbeat gap analysis (downtime per agent)\n","  - Decisions/action items per session; top files modified\n","- COO:\n","  - Throughput (sessions/day), WIP (open tasks), lead time (assign→complete), on‑time completion rate\n","  - Blockers/incidents over time; SLA (mean/95th) for responses\n","  - Utilization: active hours per agent (from heartbeats/events)\n","  - Season/Flight KPIs (when docs align)"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3"}},
 "nbformat": 4,
 "nbformat_minor": 5
}

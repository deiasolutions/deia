{
  "algorithm": "PPO",
  "model_file": "ppo_trained.zip",
  "num_episodes": 20,
  "scores": [
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0
  ],
  "episode_lengths": [
    31,
    31,
    31,
    31,
    31,
    31,
    31,
    31,
    31,
    31,
    31,
    31,
    31,
    31,
    31,
    31,
    31,
    31,
    31,
    31
  ],
  "mean_score": 0.0,
  "std_score": 0.0,
  "max_score": 0,
  "min_score": 0,
  "mean_episode_length": 31.0,
  "std_episode_length": 0.0,
  "total_frames": 620
}